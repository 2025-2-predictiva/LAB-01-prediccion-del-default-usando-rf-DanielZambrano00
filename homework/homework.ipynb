{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9d33f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import json\n",
    "import os\n",
    "import pickle\n",
    "from typing import List, Tuple\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    balanced_accuracy_score,\n",
    "    confusion_matrix,\n",
    "    f1_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    ")\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "06717612",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rutas que usa el test\n",
    "MODEL_FILENAME = \"../files/models/model.pkl.gz\"\n",
    "METRICS_FILENAME = \"../files/output/metrics.json\"\n",
    "TRAIN_PATH = os.path.join(\"..\",\"files\", \"input\", \"train_data.csv.zip\")\n",
    "TEST_PATH = os.path.join(\"..\",\"files\", \"input\", \"test_data.csv.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ba35e3db",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 1. CARGA Y LIMPIEZA\n",
    "# ---------------------------------------------------------------------\n",
    "def load_raw_data() -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"Carga los CSV comprimidos de entrenamiento y prueba.\"\"\"\n",
    "    if not os.path.exists(TRAIN_PATH):\n",
    "        raise FileNotFoundError(f\"No encuentro {TRAIN_PATH}\")\n",
    "    if not os.path.exists(TEST_PATH):\n",
    "        raise FileNotFoundError(f\"No encuentro {TEST_PATH}\")\n",
    "\n",
    "    # pandas detecta que están comprimidos por la extensión .zip\n",
    "    df_train = pd.read_csv(TRAIN_PATH)\n",
    "    df_test = pd.read_csv(TEST_PATH)\n",
    "    return df_train, df_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c1edb5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_dataset(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Aplica las transformaciones del Paso 1.\"\"\"\n",
    "    # Renombrar variable objetivo\n",
    "    if \"default payment next month\" in df.columns:\n",
    "        df = df.rename(columns={\"default payment next month\": \"default\"})\n",
    "\n",
    "    # Quitar columna ID si existe\n",
    "    if \"ID\" in df.columns:\n",
    "        df = df.drop(columns=[\"ID\"])\n",
    "\n",
    "    # EDUCATION > 4 -> 4 (categoría others)\n",
    "    if \"EDUCATION\" in df.columns:\n",
    "        df.loc[df[\"EDUCATION\"] > 4, \"EDUCATION\"] = 4\n",
    "\n",
    "    # Eliminar filas con NA\n",
    "    df = df.dropna(axis=0, how=\"any\").reset_index(drop=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "245e953a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_xy(df: pd.DataFrame) -> Tuple[pd.DataFrame, pd.Series]:\n",
    "    \"\"\"Separa X e y (Paso 2).\"\"\"\n",
    "    if \"default\" not in df.columns:\n",
    "        raise RuntimeError('La columna objetivo \"default\" no existe en el dataset.')\n",
    "\n",
    "    y = df[\"default\"].astype(int)\n",
    "    X = df.drop(columns=[\"default\"])\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "98478b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. PIPELINE + GRID SEARCH\n",
    "# ---------------------------------------------------------------------\n",
    "def build_model(x_train: pd.DataFrame) -> GridSearchCV:\n",
    "    \"\"\"\n",
    "    Crea el pipeline y el GridSearchCV (Pasos 3 y 4), alineado con el notebook:\n",
    "\n",
    "    Pipeline:\n",
    "      - ColumnTransformer con OneHotEncoder para columnas categóricas\n",
    "      - RandomForestClassifier como estimador final (paso 'classifier')\n",
    "    \"\"\"\n",
    "\n",
    "    # Columnas categóricas exactamente como en el taller\n",
    "    categorical_features: List[str] = []\n",
    "    for col in [\"SEX\", \"EDUCATION\", \"MARRIAGE\"]:\n",
    "        if col in x_train.columns:\n",
    "            categorical_features.append(col)\n",
    "\n",
    "    # El resto se considera numérico\n",
    "    numeric_features = [c for c in x_train.columns if c not in categorical_features]\n",
    "\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\n",
    "                \"cat\",\n",
    "                OneHotEncoder(handle_unknown=\"ignore\"),\n",
    "                categorical_features,\n",
    "            ),\n",
    "            (\"num\", \"passthrough\", numeric_features),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    rf = RandomForestClassifier(\n",
    "        random_state=42,\n",
    "        n_jobs=-1,  # opcional, solo para acelerar\n",
    "    )\n",
    "\n",
    "    # OJO: nombres de pasos como en el notebook: 'preprocessor' y 'classifier'\n",
    "    pipeline = Pipeline(\n",
    "        steps=[\n",
    "            (\"preprocessor\", preprocessor),\n",
    "            (\"classifier\", rf),\n",
    "        ],\n",
    "        verbose=False,\n",
    "    )\n",
    "\n",
    "    # Rejilla de hiperparámetros igual a la de tu compañero\n",
    "    param_grid = {\n",
    "        \"classifier__n_estimators\": [100, 200],\n",
    "        \"classifier__max_depth\": [None, 10, 20],\n",
    "        \"classifier__min_samples_split\": [2, 5],\n",
    "        \"classifier__min_samples_leaf\": [1, 2],\n",
    "    }\n",
    "\n",
    "    grid = GridSearchCV(\n",
    "        estimator=pipeline,\n",
    "        param_grid=param_grid,\n",
    "        cv=10,\n",
    "        scoring=\"balanced_accuracy\",\n",
    "        n_jobs=-1,\n",
    "        refit=True,\n",
    "        verbose=0,\n",
    "    )\n",
    "\n",
    "    return grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "532ad092",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. MÉTRICAS\n",
    "# ---------------------------------------------------------------------\n",
    "def compute_classification_metrics(y_true, y_pred, dataset_name: str) -> dict:\n",
    "    \"\"\"Diccionario con las métricas que pide el test (Paso 6).\"\"\"\n",
    "    return {\n",
    "        \"type\": \"metrics\",\n",
    "        \"dataset\": dataset_name,\n",
    "        \"precision\": float(precision_score(y_true, y_pred, zero_division=0)),\n",
    "        \"balanced_accuracy\": float(balanced_accuracy_score(y_true, y_pred)),\n",
    "        \"recall\": float(recall_score(y_true, y_pred, zero_division=0)),\n",
    "        \"f1_score\": float(f1_score(y_true, y_pred, zero_division=0)),\n",
    "    }\n",
    "\n",
    "\n",
    "def compute_confusion_matrix_dict(y_true, y_pred, dataset_name: str) -> dict:\n",
    "    \"\"\"Diccionario con la matriz de confusión en el formato del Paso 7.\"\"\"\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=[0, 1])\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "    return {\n",
    "        \"type\": \"cm_matrix\",\n",
    "        \"dataset\": dataset_name,\n",
    "        \"true_0\": {\"predicted_0\": int(tn), \"predicted_1\": int(fp)},\n",
    "        \"true_1\": {\"predicted_0\": int(fn), \"predicted_1\": int(tp)},\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0757bdc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. GUARDAR MODELO Y MÉTRICAS\n",
    "# ---------------------------------------------------------------------\n",
    "def save_model(model) -> None:\n",
    "    os.makedirs(os.path.dirname(MODEL_FILENAME), exist_ok=True)\n",
    "    with gzip.open(MODEL_FILENAME, \"wb\") as f:\n",
    "        pickle.dump(model, f)\n",
    "\n",
    "\n",
    "def save_metrics(records) -> None:\n",
    "    \"\"\"\n",
    "    records: lista de diccionarios.\n",
    "    Orden esperado por el test:\n",
    "      0 -> métricas train\n",
    "      1 -> métricas test\n",
    "      2 -> matriz confusión train\n",
    "      3 -> matriz confusión test\n",
    "    \"\"\"\n",
    "    os.makedirs(os.path.dirname(METRICS_FILENAME), exist_ok=True)\n",
    "    with open(METRICS_FILENAME, \"w\", encoding=\"utf-8\") as f:\n",
    "        for rec in records:\n",
    "            f.write(json.dumps(rec) + \"\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1e3ab182",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. MAIN\n",
    "# ---------------------------------------------------------------------\n",
    "def main() -> None:\n",
    "    # Paso 1: cargar y limpiar\n",
    "    df_train_raw, df_test_raw = load_raw_data()\n",
    "    df_train = clean_dataset(df_train_raw)\n",
    "    df_test = clean_dataset(df_test_raw)\n",
    "\n",
    "    # Paso 2: dividir en X/y\n",
    "    x_train, y_train = split_xy(df_train)\n",
    "    x_test, y_test = split_xy(df_test)\n",
    "\n",
    "    # Pasos 3 y 4: pipeline + GridSearchCV\n",
    "    model = build_model(x_train)\n",
    "    model.fit(x_train, y_train)\n",
    "\n",
    "    # Paso 5: guardar modelo\n",
    "    save_model(model)\n",
    "\n",
    "    # Paso 6: métricas\n",
    "    y_pred_train = model.predict(x_train)\n",
    "    y_pred_test = model.predict(x_test)\n",
    "\n",
    "    metrics_train = compute_classification_metrics(y_train, y_pred_train, \"train\")\n",
    "    metrics_test = compute_classification_metrics(y_test, y_pred_test, \"test\")\n",
    "\n",
    "    # Paso 7: matrices de confusión\n",
    "    cm_train = compute_confusion_matrix_dict(y_train, y_pred_train, \"train\")\n",
    "    cm_test = compute_confusion_matrix_dict(y_test, y_pred_test, \"test\")\n",
    "\n",
    "    # Guardar en metrics.json\n",
    "    save_metrics([metrics_train, metrics_test, cm_train, cm_test])\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e231795",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff182ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
